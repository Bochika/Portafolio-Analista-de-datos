{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "f41ffcd5-782b-47b5-bec2-9f5cacf2f3b0",
      "cell_type": "code",
      "source": "# Utilizaci√≥n de pandas  en el an√°lisis de datos",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "e4732683-b4ef-406a-afb5-f4650d82a3a6",
      "cell_type": "code",
      "source": "## poder subir un archivo, en este caso .cvs ubicado en un archivo local, Le hacemos una primera mirada de los primeros y los √∫ltimos datos para ver como est√° compuesta, vemos informaci√≥n importante y si hay duplicados.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "5fb1c683-161e-499a-a2b0-8f0c2c5d4099",
      "cell_type": "code",
      "source": "import pandas as pd\nimport os \nfrom collections import defaultdict\n\n\nruta_directorio = r\"C:\\Users\\luism\\.cache\\kagglehub\\datasets\\ayeshaimran123\\caffeine-collective\\versions\\1\"\nnombre_archivo = \"Coffe_sales.csv\" \nruta_completa = os.path.join(ruta_directorio, nombre_archivo)\n\ntry:\n    df = pd.read_csv(ruta_completa)\n    print(\"¬°Archivo cargado exitosamente!\")\n    print(\"\\nPrimeras 5 filas del DataFrame:\")\n    print(df.head())\n    print(df.tail())\n    df.info()\n    df.describe()\n    df.duplicated().sum()\n\nexcept FileNotFoundError:\n    print(f\"Error: No se encontr√≥ el archivo en la ruta: {ruta_completa}\")\n   \nexcept Exception as e:\n    print(f\"Ocurri√≥ un error al leer el archivo: {e}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Error: No se encontr√≥ el archivo en la ruta: C:\\Users\\luism\\.cache\\kagglehub\\datasets\\ayeshaimran123\\caffeine-collective\\versions\\1/Coffe_sales.csv\n"
        }
      ],
      "execution_count": 6
    },
    {
      "id": "0b6869c6-b4c5-4c8f-ad62-a710f111ec63",
      "cell_type": "code",
      "source": "\n\n# Se le hizo un an√°lisis al conjunto, no tiene datos faltantes o extra√±os\n#Tenemos problemas con el formato de los tiempos\n\n\ndf['Full_Timestamp_Str'] = df['Date'].astype(str) + ' ' + df['Time'].astype(str)\n\n# Para cuantificar errores\ndf['Transaction_DateTime'] = pd.to_datetime(df['Full_Timestamp_Str'], \n                                             errors='coerce', \n                                             format='mixed')\n\n# Validar la limpieza y eliminar la columna temporal\nnan_count = df['Transaction_DateTime'].isna().sum()\nprint(\"\\n--- ¬°ERROR DE TIEMPO CORREGIDO Y VALIDADO! ---\")\nprint(f\"Total de valores que fallaron en la conversi√≥n (NaT): {nan_count}\")\n\n# Limpieza del DataFrame\ndf = df.drop(columns=['Full_Timestamp_Str'])\n\n# Validar la nueva estructura del DataFrame\nprint(\"\\nValidaci√≥n de las primeras 5 filas con el nuevo campo de tiempo:\")\nprint(df[['Date', 'Time', 'Transaction_DateTime']].head())\ndf.info()\n\n# Iterar sobre las columnas de tipo 'object' (categ√≥ricas) para ver los valores √∫nicos\nprint(\"\\n--- Inspecci√≥n de Valores √önicos en Columnas Categ√≥ricas ---\")\nfor col in df.select_dtypes(include=['object']).columns:\n    print(f\"\\nColumna: {col}\")\n    \n    # Muestra los valores √∫nicos y su frecuencia de aparici√≥n\n    print(df[col].value_counts())\n    \n    \n# Ya los datos est√°n totalmente limpios y completos\n\n# Ahora vamos a extraer informaci√≥n valiosa con ayuda de pandas\n\ndias_ordenados = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n\ndemanda_cruzada = pd.crosstab(\n    df['Weekday'], \n    df['Time_of_Day']\n).reindex(dias_ordenados)\n\nprint(\"\\n--- Demanda Semanal por Momento del D√≠a (Conteo de Ventas) ---\")\nprint(demanda_cruzada)\n\n# Agrupamos por Momento del D√≠a y por Nombre del Caf√©, y contamos las ventas\nventas_por_momento_y_cafe = df.groupby(['Time_of_Day', 'coffee_name']).size()\n\n# Usamos 'idxmax' en el nivel interior (coffee_name) para encontrar el √≠ndice (nombre del caf√©)\n# que tuvo la mayor venta en cada Time_of_Day.\ncafe_lider_por_momento = ventas_por_momento_y_cafe.groupby(level=0).idxmax()\n\nprint(\"\\n--- Caf√© L√≠der de Ventas en Cada Momento del D√≠a ---\")\nprint(cafe_lider_por_momento)\n\n# Inicializar y generar el diccionario (Repetimos la l√≥gica del paso anterior para asegurar la ejecuci√≥n)\nreportes_por_cafe = defaultdict(pd.DataFrame)\n\nfor cafe in df['coffee_name'].unique():\n    # Filtrar el DataFrame original\n    df_filtrado = df[df['coffee_name'] == cafe]\n    \n    # Crear la Tabla Din√°mica (Pivot Table)\n    pivot_cafe = df_filtrado.pivot_table(\n        index='Time_of_Day', \n        columns='Weekday', \n        values='hour_of_day', \n        aggfunc='count'\n    )\n    \n    # Reordenar las columnas y rellenar los posibles valores NaN (cero ventas) con 0\n    pivot_cafe = pivot_cafe.reindex(columns=dias_ordenados, fill_value=0)\n    \n    reportes_por_cafe[cafe] = pivot_cafe\n\n\n# 1. Bucle de Impresi√≥n de Todos los Reportes (La Solicitud Clave)\nprint(\"\\n\" + \"=\"*50)\nprint(\"     REPORTE COMPLETO DE VENTAS POR PRODUCTO Y DEMANDA\")\nprint(\"=\"*50)\n\n# Iterar sobre el diccionario e imprimir cada DataFrame de la Tabla Din√°mica\nfor nombre_cafe, reporte_df in reportes_por_cafe.items():\n    print(f\"\\n--- ‚òï REPORTE: {nombre_cafe} ---\")\n    print(reporte_df)\n    print(\"-\" * 50)\n\n\n# Para sacar m√©tricas seguras y poder ser certeros con los datos, debemos ver que los datos por d√≠as sean equitativos.\n# Puede que hayan m√°s lunes que jueves, llevando a lunes a tener una mayor percepci√≥n, que podr√≠a ser falsa.\n\n# 1. Ventas Totales por D√≠a: Contar el n√∫mero de transacciones por cada d√≠a de la semana\nventas_totales_por_dia = df.groupby('Weekday').size().reindex(dias_ordenados, fill_value=0)\n\n# 2. Conteo de Ocurrencias del D√≠a (El paso clave para la normalizaci√≥n):\n# Eliminamos las filas duplicadas por 'Date' y luego contamos cu√°ntas veces aparece cada 'Weekday'.\n# Esto nos dice cu√°ntos Lunes, Martes, etc., hubo en el per√≠odo total.\nconteo_dias_unicos = df.drop_duplicates(subset=['Date'])['Weekday'].value_counts().reindex(dias_ordenados, fill_value=0)\n\n# 3. Calcular la Tasa Promedio de Ventas por D√≠a (M√©trica Segura)\n# Tasa Promedio = Ventas Totales / Conteo de D√≠as √önicos\ntasa_promedio_ventas = (ventas_totales_por_dia / conteo_dias_unicos).round(2).sort_values(ascending=False)\n\nprint(\"\\n--- üìà TASA PROMEDIO DE VENTAS DIARIAS NORMALIZADA ---\")\nprint(\"Este es el verdadero indicador de demanda, libre de sesgo.\")\nprint(\"-\" * 50)\nprint(tasa_promedio_ventas)\nprint(\"-\" * 50)\n\n# Opcional: Mostrar el conteo de d√≠as √∫nicos para verificar la normalizaci√≥n\nprint(\"\\nConteo de D√≠as √önicos en el Per√≠odo Analizado (Para Verificaci√≥n):\")\nprint(conteo_dias_unicos)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e77a5069-ca7e-4705-bb2c-5e311d21f667",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}